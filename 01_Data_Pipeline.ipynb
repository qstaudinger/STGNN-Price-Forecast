{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a2a508-2e5c-40da-ade2-df9cb3eaa804",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87460e5e-0174-4b74-9dd9-47fc7063ffbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/qusta100/STGNN\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import ipywidgets\n",
    "from joblib import Parallel, delayed\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import BallTree, NearestNeighbors\n",
    "\n",
    "\n",
    "# Confirm the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7080d3-a633-42e3-b8db-3c1ffad9d809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the price data folder\n",
    "folder_path = \"/gpfs/scratch/qusta100/STGNN/Data/Prices/\"\n",
    "\n",
    "# Find all CSV files matching the pattern \"*-prices.csv\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*-prices.csv\"))\n",
    "\n",
    "# List to store daily data\n",
    "df_list = []\n",
    "\n",
    "# Read each file and add it to the list\n",
    "for file in csv_files:\n",
    "    # Read: timestamp, UUID, Diesel, E5, E10\n",
    "    df_day = pd.read_csv(file, usecols=[0, 1, 2, 3, 4])\n",
    "    df_day.columns = [\"date\", \"station_uuid\", \"diesel\", \"e5\", \"e10\"]\n",
    "    df_list.append(df_day)\n",
    "\n",
    "# Combine all daily data into a single DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Ensure correct column names (optional)\n",
    "df.columns = [\"date\", \"station_uuid\", \"diesel\", \"e5\", \"e10\"]\n",
    "\n",
    "# Parse timestamps and remove timezone\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True).dt.tz_convert(None)\n",
    "\n",
    "# Sort by station and time\n",
    "df = df.sort_values([\"station_uuid\", \"date\"])\n",
    "\n",
    "# Get the overall time range (start & end)\n",
    "start = df[\"date\"].min().replace(minute=0, second=0)\n",
    "end = df[\"date\"].max().replace(minute=45, second=0)\n",
    "\n",
    "# Create 15-minute time intervals\n",
    "quarter_hours = pd.date_range(start=start, end=end, freq=\"15min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b954f2c9-6c45-42b1-a58b-b714bb3852c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all station files from March and April\n",
    "station_files = glob.glob(\"/gpfs/scratch/qusta100/Gasoline/Data/Stations/2025-0[3-4]-*-stations.csv\")\n",
    "\n",
    "stations_list = []\n",
    "for file in station_files:\n",
    "    df_station = pd.read_csv(file)\n",
    "    # Extract the date from the filename and convert to datetime\n",
    "    date_str = os.path.basename(file).split(\"-stations.csv\")[0]\n",
    "    df_station[\"day\"] = pd.to_datetime(date_str)\n",
    "    stations_list.append(df_station)\n",
    "\n",
    "# Combine all station data into a single DataFrame\n",
    "stations_all = pd.concat(stations_list, ignore_index=True)\n",
    "\n",
    "# Keep only the latest entry per station (based on the 'day' column)\n",
    "stations_latest = stations_all.sort_values(\"day\").drop_duplicates(\"uuid\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2286df78-1f56-4e31-832c-05713dabbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge price data with the latest station information\n",
    "df = df.merge(\n",
    "    stations_latest,\n",
    "    how=\"left\",\n",
    "    left_on=\"station_uuid\",\n",
    "    right_on=\"uuid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /gpfs/project/qusta100/pytorch/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# Clean coordinates\n",
    "def to_scalar(v):\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return v[0] if len(v) else np.nan\n",
    "    return v\n",
    "\n",
    "lon = pd.to_numeric(df[\"longitude\"].map(to_scalar), errors=\"coerce\")\n",
    "lat = pd.to_numeric(df[\"latitude\"].map(to_scalar),  errors=\"coerce\")\n",
    "\n",
    "# 2) Vectorize Coordinates\n",
    "geom = gpd.points_from_xy(lon, lat)  # erwartet WGS84\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df.copy(), geometry=geom, crs=\"EPSG:4326\")\n",
    "\n",
    "# 3) Load shape files\n",
    "nrw = gpd.read_file(\"/gpfs/scratch/qusta100/STGNN/Data/Shapes/thuringia.geojson\").to_crs(\"EPSG:4326\")\n",
    "\n",
    "# 4) Spatial Join\n",
    "joined = gpd.sjoin(gdf, nrw[[\"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "gdf[\"in_thuringia\"] = joined.index_right.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b) Initialize in_region: 1 for Thuringia, 0 otherwise\n",
    "gdf[\"in_region\"] = gdf[\"in_thuringia\"].fillna(False).astype(np.int8)\n",
    "\n",
    "# 5) Find neighbors within 30 km using BallTree (memory-efficient batch version)\n",
    "valid = gdf[\"latitude\"].notna() & gdf[\"longitude\"].notna()\n",
    "th_mask = gdf[\"in_thuringia\"].fillna(False) & valid\n",
    "\n",
    "if th_mask.any():\n",
    "    lat_v = gdf.loc[valid, \"latitude\"].to_numpy(dtype=np.float32)\n",
    "    lon_v = gdf.loc[valid, \"longitude\"].to_numpy(dtype=np.float32)\n",
    "    coords_rad = np.deg2rad(np.column_stack([lat_v, lon_v])).astype(np.float32, copy=False)\n",
    "\n",
    "    # Store the DataFrame indices of valid rows (to map results back later)\n",
    "    valid_idx = gdf.index[valid].to_numpy()\n",
    "\n",
    "    # Query points: only Thuringia stations\n",
    "    th_lat = gdf.loc[th_mask, \"latitude\"].to_numpy(dtype=np.float32)\n",
    "    th_lon = gdf.loc[th_mask, \"longitude\"].to_numpy(dtype=np.float32)\n",
    "    th_coords_rad = np.deg2rad(np.column_stack([th_lat, th_lon])).astype(np.float32, copy=False)\n",
    "\n",
    "    # Build BallTree using Haversine distance (spherical)\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "    # Radius in radians (30 km on Earth’s surface)\n",
    "    R_EARTH_KM = np.float32(6371.0088)\n",
    "    radius_rad = np.float32(30.0) / R_EARTH_KM\n",
    "\n",
    "    # Boolean mask for all valid rows; will be True for stations in the region\n",
    "    region_mask_valid = np.zeros(coords_rad.shape[0], dtype=bool)\n",
    "\n",
    "    # Batch query\n",
    "    BATCH = 5000\n",
    "    for start in range(0, th_coords_rad.shape[0], BATCH):\n",
    "        end = start + BATCH\n",
    "        neigh_lists = tree.query_radius(th_coords_rad[start:end], r=radius_rad)\n",
    "\n",
    "        # Instead of concatenating large arrays, mark neighbors directly in the mask\n",
    "        for arr in neigh_lists:\n",
    "            if arr.size:\n",
    "                region_mask_valid[arr] = True\n",
    "\n",
    "    # Ensure all Thuringia points themselves are included\n",
    "    region_mask_valid[np.searchsorted(valid_idx, gdf.index[th_mask])] = True\n",
    "\n",
    "    # Map back to global DataFrame indices and set in_region = 1\n",
    "    gdf.loc[valid_idx[region_mask_valid], \"in_region\"] = 1\n",
    "\n",
    "# 6) Create a plain DataFrame without geometry\n",
    "ndf = pd.DataFrame(gdf).drop(columns=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823573\n",
      "1943147\n"
     ]
    }
   ],
   "source": [
    "# Number of entries in Thuringia\n",
    "print(len(ndf[ndf[\"in_thuringia\"] == True]))\n",
    "print(len(ndf[ndf[\"in_region\"] == 1]))\n",
    "\n",
    "# Filter only entries in the wanted region\n",
    "ndf = ndf[ndf[\"in_region\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5fd91-71d4-4b1f-86c3-706cd843ab99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d341f37f-1075-490f-bbbf-2c7ef1c9e5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Result list\n",
    "resampled_list = []\n",
    "\n",
    "# List of fuel price columns\n",
    "price_cols = [\"diesel\", \"e5\", \"e10\"]\n",
    "\n",
    "# List of metadata columns (everything except date, UUID, and prices)\n",
    "metadata_cols = [col for col in ndf.columns if col not in [\"date\", \"station_uuid\"] + price_cols]\n",
    "\n",
    "# For each station: resample time series and attach metadata\n",
    "for station_id, group in ndf.groupby(\"station_uuid\"):\n",
    "    group = group.set_index(\"date\").sort_index()\n",
    "\n",
    "    # Resample prices (forward fill) for each fuel type\n",
    "    reindexed_prices = {\n",
    "        col: group[col].reindex(quarter_hours, method=\"ffill\")\n",
    "        for col in price_cols\n",
    "    }\n",
    "\n",
    "    # DataFrame with timestamps and UUID\n",
    "    result = pd.DataFrame({\n",
    "        \"date\": quarter_hours,\n",
    "        \"station_uuid\": station_id,\n",
    "        **{col: reindexed_prices[col].values for col in price_cols}\n",
    "    })\n",
    "\n",
    "    # Metadata from the first row (if available)\n",
    "    meta = group[metadata_cols].iloc[0] if not group.empty else pd.Series(index=metadata_cols)\n",
    "\n",
    "    # Attach metadata to each row\n",
    "    for col in metadata_cols:\n",
    "        result[col] = meta[col]\n",
    "\n",
    "    resampled_list.append(result)\n",
    "\n",
    "# Merge all resampled data\n",
    "resampled_df = pd.concat(resampled_list, ignore_index=True)\n",
    "\n",
    "# Restrict to April\n",
    "resampled_df = resampled_df[resampled_df[\"date\"] >= pd.Timestamp(\"2025-04-01 00:00:00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9edd1fe4-81f0-49a2-a1b6-c7344d87fa70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing metadata\n",
    "df_clean = resampled_df[resampled_df[[\"latitude\", \"longitude\"]].notna().all(axis=1)].copy()\n",
    "\n",
    "# Remove rows with all prices missing\n",
    "df_clean = df_clean[df_clean[[\"diesel\", \"e5\", \"e10\"]].notna().any(axis=1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs after filter: 2963412\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"date\"] = pd.to_datetime(df_clean[\"date\"])\n",
    "\n",
    "# 1) Last observation date per station\n",
    "last_seen = (\n",
    "    df_clean.groupby(\"station_uuid\", as_index=False)[\"date\"]\n",
    "    .max()\n",
    "    .rename(columns={\"date\": \"last_seen\"})\n",
    ")\n",
    "\n",
    "# 2) Create a new DataFrame\n",
    "df_filtered = df_clean.merge(last_seen, on=\"station_uuid\", how=\"left\")\n",
    "df_filtered[\"last_seen\"] = pd.to_datetime(df_filtered[\"last_seen\"])\n",
    "\n",
    "# 3) Moving activity and price filter\n",
    "cutoff = pd.Timedelta(days=15)\n",
    "mask_active = (df_filtered[\"date\"] - df_filtered[\"last_seen\"]) <= cutoff\n",
    "mask_price  = df_filtered[\"diesel\"] > 0\n",
    "\n",
    "df_filtered = df_filtered[mask_active & mask_price].copy()\n",
    "\n",
    "print(\"Obs after filter:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5359d050-b812-4ec5-a774-dd4fe950a659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace 0 with NaN for all price columns\n",
    "price_columns = [\"diesel\", \"e5\", \"e10\"]\n",
    "\n",
    "# Replace 0 and negative values with NA in price columns (if they exist)\n",
    "for col in price_columns:\n",
    "    if col in df_filtered.columns:\n",
    "        df_filtered[col] = df_filtered[col].apply(lambda x: pd.NA if x <= 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201f19e1-fb55-45fd-96d9-01fba67783af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handling opening times\n",
    "def is_open(opening_json, timestamp):\n",
    "    try:\n",
    "        if opening_json.strip() == \"{}\":\n",
    "            return True  # always open according to MTS-K\n",
    "\n",
    "        data = json.loads(opening_json)\n",
    "        weekday = timestamp.weekday()  # 0 = Monday, ..., 6 = Sunday\n",
    "        time_str = timestamp.strftime(\"%H:%M\")\n",
    "\n",
    "        for block in data.get(\"openingTimes\", []):\n",
    "            if block.get(\"applicable_days\", 0) & (1 << weekday):\n",
    "                for period in block.get(\"periods\", []):\n",
    "                    if period[\"startp\"] <= time_str < period[\"endp\"]:\n",
    "                        return True\n",
    "        return False  # none of the time blocks apply\n",
    "    except:\n",
    "        return True  # if uncertain: treat as open\n",
    "\n",
    "# Create a copy\n",
    "df_clean_opening = df_filtered.copy()\n",
    "\n",
    "# Price columns to set to NaN if station is closed\n",
    "price_columns = [col for col in [\"diesel\", \"e5\", \"e10\"] if col in df_clean_opening.columns]\n",
    "\n",
    "# Calculate opening status for each row\n",
    "df_clean_opening[\"is_open\"] = df_clean_opening.apply(\n",
    "    lambda row: is_open(row[\"openingtimes_json\"], row[\"date\"]), axis=1\n",
    ")\n",
    "\n",
    "# Set all prices to NaN if closed\n",
    "for col in price_columns:\n",
    "    df_clean_opening.loc[~df_clean_opening[\"is_open\"], col] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected highway stations: 452\n",
      "Rows before exclusion: 2963412\n",
      "Rows after exclusion: 2868636\n"
     ]
    }
   ],
   "source": [
    "# Verification and exclusion of highway stations using the stations dataset\n",
    "autobahn_pattern = (\n",
    "    r\"\\b(?:\"\n",
    "    r\"autobahn(?:\\s*tankstelle)?\"\n",
    "    r\"|autohof\"\n",
    "    r\"|rasthof\"\n",
    "    r\"|rast[-\\s]?stätte\"\n",
    "    r\"|raststaette\"\n",
    "    r\"|tank\\s*(?:und|&)\\s*rast\"\n",
    "    r\"|service\\s*area\"\n",
    "    r\"|an\\s+der\\s+a[0-9]{1,2}\"\n",
    "    r\"|a\\s*[0-9]{1,2}\"\n",
    "    r\"|ausfahrt\"\n",
    "    r\"|abfahrt\"\n",
    "    r\"|bundesautobahn\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "# 2. Identify highway stations by name or street\n",
    "is_highway = (\n",
    "    stations_latest[\"name\"].str.contains(autobahn_pattern, case=False, na=False) |\n",
    "    stations_latest[\"street\"].str.contains(autobahn_pattern, case=False, na=False)\n",
    ")\n",
    "\n",
    "# 3. Get UUIDs of identified highway stations\n",
    "highway_ids = stations_latest[is_highway][\"uuid\"].unique()\n",
    "\n",
    "# 4. Exclude these stations from df_clean_opening\n",
    "df_clean_opening_excl_highway = df_clean_opening[\n",
    "    ~df_clean_opening[\"station_uuid\"].isin(highway_ids)\n",
    "].copy()\n",
    "\n",
    "# 5. Output\n",
    "print(\"Number of detected highway stations:\", len(highway_ids))\n",
    "print(\"Rows before exclusion:\", len(df_clean_opening))\n",
    "print(\"Rows after exclusion:\", len(df_clean_opening_excl_highway))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date  weekday  holiday   time\n",
      "0 2025-04-01 00:00:00  Tuesday        0  00:00\n",
      "1 2025-04-01 00:15:00  Tuesday        0  00:15\n",
      "2 2025-04-01 00:30:00  Tuesday        0  00:30\n",
      "3 2025-04-01 00:45:00  Tuesday        0  00:45\n",
      "4 2025-04-01 01:00:00  Tuesday        0  01:00\n"
     ]
    }
   ],
   "source": [
    "# Add Features\n",
    "df = df_clean_opening_excl_highway.copy()\n",
    "\n",
    "# Assuming your DataFrame is named df and the timestamp column is 'date'\n",
    "s = df['date'].astype(str) \\\n",
    "              .str.replace('\\u00a0', ' ', regex=False) \\\n",
    "              .str.strip() \\\n",
    "              .str.replace(r'\\s+', ' ', regex=True)\n",
    "parsed_full  = pd.to_datetime(s, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "parsed_date  = pd.to_datetime(s, format='%Y-%m-%d', errors='coerce')\n",
    "df['date']   = parsed_full.fillna(parsed_date)\n",
    "\n",
    "# 1) Weekday in German (Monday–Sunday)\n",
    "# Method A: using German locale (works if installed on your system)\n",
    "df['weekday'] = df['date'].dt.day_name(locale='de_DE')\n",
    "\n",
    "# Alternatively, if the locale isn't available, manual mapping:\n",
    "weekday_map = {\n",
    "    0: 'Monday',\n",
    "    1: 'Tuesday',\n",
    "    2: 'Wednesday',\n",
    "    3: 'Thursday',\n",
    "    4: 'Friday',\n",
    "    5: 'Saturday',\n",
    "    6: 'Sunday'\n",
    "}\n",
    "df['weekday'] = df['date'].dt.weekday.map(weekday_map)\n",
    "\n",
    "# 2) Holiday dummy (1 if date is 2025-04-18 or 2025-04-21, otherwise 0)\n",
    "holidays = {pd.Timestamp('2025-04-18'), pd.Timestamp('2025-04-21')}\n",
    "# .normalize() resets the time to midnight so you compare only dates\n",
    "df['holiday'] = df['date'].dt.normalize().isin(holidays).astype(int)\n",
    "\n",
    "# 3) Time formatted as \"hh:mm\"\n",
    "df['time'] = df['date'].dt.strftime('%H:%M')\n",
    "\n",
    "# Example output\n",
    "print(df[['date', 'weekday', 'holiday', 'time']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw oil prices via API\n",
    "# Because of a missing network connection here, this step was run on a different server\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Retrieve hourly Brent data from April 1 to April 30, 2025\n",
    "ticker = yf.Ticker(\"BZ=F\")\n",
    "df = ticker.history(start=\"2025-04-01\", end=\"2025-05-01\", interval=\"60m\")\n",
    "df = df.reset_index()\n",
    "df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
    "\n",
    "# 2. Generate a complete 15-minute timestamp grid for April 2025\n",
    "all_times = pd.date_range(\n",
    "    start=\"2025-04-01 00:00:00\",\n",
    "    end=\"2025-04-30 23:45:00\",\n",
    "    freq=\"15min\"\n",
    ")\n",
    "full_df = pd.DataFrame({'Datetime': all_times})\n",
    "\n",
    "# 3. Select only the 'Open' price and rename it to 'Brent_Price'\n",
    "df_short = df[['Datetime', 'Open']].rename(columns={'Open': 'Brent_Price'})\n",
    "\n",
    "# 4. Merge the full 15-minute grid with the hourly prices,\n",
    "#    filling each 15-min slot with the most recent (backward fill) hourly price\n",
    "merged = pd.merge_asof(\n",
    "    full_df.sort_values('Datetime'),\n",
    "    df_short.sort_values('Datetime'),\n",
    "    on='Datetime',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# 5. Save the result to a CSV file\n",
    "merged.to_csv(\"brent_april_2025_15min_filled.csv\", index=False)\n",
    "\n",
    "# Optional: display the first 20 rows and total count (should be 2880 timestamps)\n",
    "print(merged.head(20))\n",
    "print(\"Total number of timestamps:\", len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (restore from saved file)\n",
    "brent = pd.read_csv(\"/gpfs/scratch/qusta100/STGNN/Data/Temp/brent_april_2025_15min_filled.csv\")\n",
    "brent = brent.rename(columns={\"Datetime\": \"date\"})\n",
    "brent['date'] = pd.to_datetime(brent['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_uuid</th>\n",
       "      <th>diesel</th>\n",
       "      <th>e5</th>\n",
       "      <th>e10</th>\n",
       "      <th>uuid</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>street</th>\n",
       "      <th>house_number</th>\n",
       "      <th>...</th>\n",
       "      <th>openingtimes_json</th>\n",
       "      <th>day</th>\n",
       "      <th>in_thuringia</th>\n",
       "      <th>in_region</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>is_open</th>\n",
       "      <th>weekday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>time</th>\n",
       "      <th>Brent_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>00:00</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-01 00:15:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>00:15</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-01 00:30:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>00:30</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-01 00:45:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>00:45</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>01:00</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                          station_uuid  diesel     e5  \\\n",
       "0 2025-04-01 00:00:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "1 2025-04-01 00:15:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "2 2025-04-01 00:30:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "3 2025-04-01 00:45:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "4 2025-04-01 01:00:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "\n",
       "     e10                                  uuid               name  \\\n",
       "0  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "1  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "2  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "3  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "4  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "\n",
       "                    brand            street house_number  ...  \\\n",
       "0  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...   \n",
       "1  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...   \n",
       "2  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...   \n",
       "3  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...   \n",
       "4  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...   \n",
       "\n",
       "  openingtimes_json        day  in_thuringia  in_region           last_seen  \\\n",
       "0                {} 2025-04-30         False          1 2025-04-30 21:45:00   \n",
       "1                {} 2025-04-30         False          1 2025-04-30 21:45:00   \n",
       "2                {} 2025-04-30         False          1 2025-04-30 21:45:00   \n",
       "3                {} 2025-04-30         False          1 2025-04-30 21:45:00   \n",
       "4                {} 2025-04-30         False          1 2025-04-30 21:45:00   \n",
       "\n",
       "  is_open  weekday  holiday   time Brent_Price  \n",
       "0    True  Tuesday        0  00:00   74.959999  \n",
       "1    True  Tuesday        0  00:15   74.959999  \n",
       "2    True  Tuesday        0  00:30   74.959999  \n",
       "3    True  Tuesday        0  00:45   74.959999  \n",
       "4    True  Tuesday        0  01:00   74.959999  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with main data set (df_main)\n",
    "final_df = pd.merge(df, brent, on='date', how='left')\n",
    "\n",
    "# Show data set\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataframe to CSV\n",
    "final_df.to_csv(\"/gpfs/scratch/qusta100/STGNN/Data/Temp/final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
