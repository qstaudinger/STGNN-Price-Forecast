{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a2a508-2e5c-40da-ade2-df9cb3eaa804",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87460e5e-0174-4b74-9dd9-47fc7063ffbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/qusta100/STGNN\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import ipywidgets\n",
    "from joblib import Parallel, delayed\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import BallTree, NearestNeighbors\n",
    "\n",
    "\n",
    "# Confirm the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7080d3-a633-42e3-b8db-3c1ffad9d809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the price data folder\n",
    "folder_path = \"/gpfs/scratch/qusta100/STGNN/Data/Prices/\"\n",
    "\n",
    "# Find all CSV files matching the pattern \"*-prices.csv\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*-prices.csv\"))\n",
    "\n",
    "# List to store daily data\n",
    "df_list = []\n",
    "\n",
    "# Read each file and add it to the list\n",
    "for file in csv_files:\n",
    "    # Read: timestamp, UUID, Diesel, E5, E10\n",
    "    df_day = pd.read_csv(file, usecols=[0, 1, 2, 3, 4])\n",
    "    df_day.columns = [\"date\", \"station_uuid\", \"diesel\", \"e5\", \"e10\"]\n",
    "    df_list.append(df_day)\n",
    "\n",
    "# Combine all daily data into a single DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Ensure correct column names (optional)\n",
    "df.columns = [\"date\", \"station_uuid\", \"diesel\", \"e5\", \"e10\"]\n",
    "\n",
    "# Parse timestamps and remove timezone\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True).dt.tz_convert(None)\n",
    "\n",
    "# Sort by station and time\n",
    "df = df.sort_values([\"station_uuid\", \"date\"])\n",
    "\n",
    "# Get the overall time range (start & end)\n",
    "start = df[\"date\"].min().replace(minute=0, second=0)\n",
    "end = df[\"date\"].max().replace(minute=45, second=0)\n",
    "\n",
    "# Create 15-minute time intervals\n",
    "quarter_hours = pd.date_range(start=start, end=end, freq=\"15min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b954f2c9-6c45-42b1-a58b-b714bb3852c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all station files from March and April\n",
    "station_files = glob.glob(\"/gpfs/scratch/qusta100/Gasoline/Data/Stations/2025-0[3-4]-*-stations.csv\")\n",
    "\n",
    "stations_list = []\n",
    "for file in station_files:\n",
    "    df_station = pd.read_csv(file)\n",
    "    # Extract the date from the filename and convert to datetime\n",
    "    date_str = os.path.basename(file).split(\"-stations.csv\")[0]\n",
    "    df_station[\"day\"] = pd.to_datetime(date_str)\n",
    "    stations_list.append(df_station)\n",
    "\n",
    "# Combine all station data into a single DataFrame\n",
    "stations_all = pd.concat(stations_list, ignore_index=True)\n",
    "\n",
    "# Keep only the latest entry per station (based on the 'day' column)\n",
    "stations_latest = stations_all.sort_values(\"day\").drop_duplicates(\"uuid\", keep=\"last\")\n",
    "\n",
    "# Delete var Day\n",
    "stations_latest = stations_latest.drop(columns=[\"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2286df78-1f56-4e31-832c-05713dabbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge price data with the latest station information\n",
    "df = df.merge(\n",
    "    stations_latest,\n",
    "    how=\"left\",\n",
    "    left_on=\"station_uuid\",\n",
    "    right_on=\"uuid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_uuid</th>\n",
       "      <th>diesel</th>\n",
       "      <th>e5</th>\n",
       "      <th>e10</th>\n",
       "      <th>uuid</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>street</th>\n",
       "      <th>house_number</th>\n",
       "      <th>post_code</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>first_active</th>\n",
       "      <th>openingtimes_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01 04:26:26</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>1.619</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.709</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>Beducker - Qualität günstig tanken</td>\n",
       "      <td>Beducker</td>\n",
       "      <td>Donauwörther Str.</td>\n",
       "      <td>47a</td>\n",
       "      <td>86405</td>\n",
       "      <td>Meitingen</td>\n",
       "      <td>48.555683</td>\n",
       "      <td>10.850848</td>\n",
       "      <td>2017-04-07 00:00:26+02</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01 05:46:31</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>1.649</td>\n",
       "      <td>1.799</td>\n",
       "      <td>1.739</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>Beducker - Qualität günstig tanken</td>\n",
       "      <td>Beducker</td>\n",
       "      <td>Donauwörther Str.</td>\n",
       "      <td>47a</td>\n",
       "      <td>86405</td>\n",
       "      <td>Meitingen</td>\n",
       "      <td>48.555683</td>\n",
       "      <td>10.850848</td>\n",
       "      <td>2017-04-07 00:00:26+02</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01 06:02:42</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>1.729</td>\n",
       "      <td>1.859</td>\n",
       "      <td>1.799</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>Beducker - Qualität günstig tanken</td>\n",
       "      <td>Beducker</td>\n",
       "      <td>Donauwörther Str.</td>\n",
       "      <td>47a</td>\n",
       "      <td>86405</td>\n",
       "      <td>Meitingen</td>\n",
       "      <td>48.555683</td>\n",
       "      <td>10.850848</td>\n",
       "      <td>2017-04-07 00:00:26+02</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01 06:30:06</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>1.729</td>\n",
       "      <td>1.849</td>\n",
       "      <td>1.789</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>Beducker - Qualität günstig tanken</td>\n",
       "      <td>Beducker</td>\n",
       "      <td>Donauwörther Str.</td>\n",
       "      <td>47a</td>\n",
       "      <td>86405</td>\n",
       "      <td>Meitingen</td>\n",
       "      <td>48.555683</td>\n",
       "      <td>10.850848</td>\n",
       "      <td>2017-04-07 00:00:26+02</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01 06:38:13</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>1.729</td>\n",
       "      <td>1.839</td>\n",
       "      <td>1.779</td>\n",
       "      <td>00006210-0037-4444-8888-acdc00006210</td>\n",
       "      <td>Beducker - Qualität günstig tanken</td>\n",
       "      <td>Beducker</td>\n",
       "      <td>Donauwörther Str.</td>\n",
       "      <td>47a</td>\n",
       "      <td>86405</td>\n",
       "      <td>Meitingen</td>\n",
       "      <td>48.555683</td>\n",
       "      <td>10.850848</td>\n",
       "      <td>2017-04-07 00:00:26+02</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                          station_uuid  diesel     e5  \\\n",
       "0 2025-03-01 04:26:26  00006210-0037-4444-8888-acdc00006210   1.619  1.769   \n",
       "1 2025-03-01 05:46:31  00006210-0037-4444-8888-acdc00006210   1.649  1.799   \n",
       "2 2025-03-01 06:02:42  00006210-0037-4444-8888-acdc00006210   1.729  1.859   \n",
       "3 2025-03-01 06:30:06  00006210-0037-4444-8888-acdc00006210   1.729  1.849   \n",
       "4 2025-03-01 06:38:13  00006210-0037-4444-8888-acdc00006210   1.729  1.839   \n",
       "\n",
       "     e10                                  uuid  \\\n",
       "0  1.709  00006210-0037-4444-8888-acdc00006210   \n",
       "1  1.739  00006210-0037-4444-8888-acdc00006210   \n",
       "2  1.799  00006210-0037-4444-8888-acdc00006210   \n",
       "3  1.789  00006210-0037-4444-8888-acdc00006210   \n",
       "4  1.779  00006210-0037-4444-8888-acdc00006210   \n",
       "\n",
       "                                 name     brand             street  \\\n",
       "0  Beducker - Qualität günstig tanken  Beducker  Donauwörther Str.   \n",
       "1  Beducker - Qualität günstig tanken  Beducker  Donauwörther Str.   \n",
       "2  Beducker - Qualität günstig tanken  Beducker  Donauwörther Str.   \n",
       "3  Beducker - Qualität günstig tanken  Beducker  Donauwörther Str.   \n",
       "4  Beducker - Qualität günstig tanken  Beducker  Donauwörther Str.   \n",
       "\n",
       "  house_number post_code       city   latitude  longitude  \\\n",
       "0          47a     86405  Meitingen  48.555683  10.850848   \n",
       "1          47a     86405  Meitingen  48.555683  10.850848   \n",
       "2          47a     86405  Meitingen  48.555683  10.850848   \n",
       "3          47a     86405  Meitingen  48.555683  10.850848   \n",
       "4          47a     86405  Meitingen  48.555683  10.850848   \n",
       "\n",
       "             first_active openingtimes_json  \n",
       "0  2017-04-07 00:00:26+02                {}  \n",
       "1  2017-04-07 00:00:26+02                {}  \n",
       "2  2017-04-07 00:00:26+02                {}  \n",
       "3  2017-04-07 00:00:26+02                {}  \n",
       "4  2017-04-07 00:00:26+02                {}  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /gpfs/project/qusta100/pytorch/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# Clean coordinates\n",
    "def to_scalar(v):\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return v[0] if len(v) else np.nan\n",
    "    return v\n",
    "\n",
    "lon = pd.to_numeric(df[\"longitude\"].map(to_scalar), errors=\"coerce\")\n",
    "lat = pd.to_numeric(df[\"latitude\"].map(to_scalar),  errors=\"coerce\")\n",
    "\n",
    "# 2) Vectorize Coordinates\n",
    "geom = gpd.points_from_xy(lon, lat)  # erwartet WGS84\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df.copy(), geometry=geom, crs=\"EPSG:4326\")\n",
    "\n",
    "# 3) Load shape files\n",
    "nrw = gpd.read_file(\"/gpfs/scratch/qusta100/STGNN/Data/Shapes/thuringia.geojson\").to_crs(\"EPSG:4326\")\n",
    "\n",
    "# 4) Spatial Join\n",
    "joined = gpd.sjoin(gdf, nrw[[\"geometry\"]], how=\"left\", predicate=\"within\")\n",
    "gdf[\"in_thuringia\"] = joined.index_right.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b) Initialize in_region: 1 for Thuringia, 0 otherwise\n",
    "gdf[\"in_region\"] = gdf[\"in_thuringia\"].fillna(False).astype(np.int8)\n",
    "\n",
    "# 5) Find neighbors within 30 km using BallTree (memory-efficient batch version)\n",
    "valid = gdf[\"latitude\"].notna() & gdf[\"longitude\"].notna()\n",
    "th_mask = gdf[\"in_thuringia\"].fillna(False) & valid\n",
    "\n",
    "if th_mask.any():\n",
    "    lat_v = gdf.loc[valid, \"latitude\"].to_numpy(dtype=np.float32)\n",
    "    lon_v = gdf.loc[valid, \"longitude\"].to_numpy(dtype=np.float32)\n",
    "    coords_rad = np.deg2rad(np.column_stack([lat_v, lon_v])).astype(np.float32, copy=False)\n",
    "\n",
    "    # Store the DataFrame indices of valid rows (to map results back later)\n",
    "    valid_idx = gdf.index[valid].to_numpy()\n",
    "\n",
    "    # Query points: only Thuringia stations\n",
    "    th_lat = gdf.loc[th_mask, \"latitude\"].to_numpy(dtype=np.float32)\n",
    "    th_lon = gdf.loc[th_mask, \"longitude\"].to_numpy(dtype=np.float32)\n",
    "    th_coords_rad = np.deg2rad(np.column_stack([th_lat, th_lon])).astype(np.float32, copy=False)\n",
    "\n",
    "    # Build BallTree using Haversine distance (spherical)\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "    # Radius in radians (30 km on Earth’s surface)\n",
    "    R_EARTH_KM = np.float32(6371.0088)\n",
    "    radius_rad = np.float32(30.0) / R_EARTH_KM\n",
    "\n",
    "    # Boolean mask for all valid rows; will be True for stations in the region\n",
    "    region_mask_valid = np.zeros(coords_rad.shape[0], dtype=bool)\n",
    "\n",
    "    # Batch query\n",
    "    BATCH = 5000\n",
    "    for start in range(0, th_coords_rad.shape[0], BATCH):\n",
    "        end = start + BATCH\n",
    "        neigh_lists = tree.query_radius(th_coords_rad[start:end], r=radius_rad)\n",
    "\n",
    "        # Instead of concatenating large arrays, mark neighbors directly in the mask\n",
    "        for arr in neigh_lists:\n",
    "            if arr.size:\n",
    "                region_mask_valid[arr] = True\n",
    "\n",
    "    # Ensure all Thuringia points themselves are included\n",
    "    region_mask_valid[np.searchsorted(valid_idx, gdf.index[th_mask])] = True\n",
    "\n",
    "    # Map back to global DataFrame indices and set in_region = 1\n",
    "    gdf.loc[valid_idx[region_mask_valid], \"in_region\"] = 1\n",
    "\n",
    "# 6) Create a plain DataFrame without geometry\n",
    "ndf = pd.DataFrame(gdf).drop(columns=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823573\n",
      "1943147\n"
     ]
    }
   ],
   "source": [
    "# Number of entries in Thuringia\n",
    "print(len(ndf[ndf[\"in_thuringia\"] == True]))\n",
    "print(len(ndf[ndf[\"in_region\"] == 1]))\n",
    "\n",
    "# Filter only entries in the wanted region\n",
    "ndf = ndf[ndf[\"in_region\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5fd91-71d4-4b1f-86c3-706cd843ab99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d341f37f-1075-490f-bbbf-2c7ef1c9e5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Result list\n",
    "resampled_list = []\n",
    "\n",
    "# List of fuel price columns\n",
    "price_cols = [\"diesel\", \"e5\", \"e10\"]\n",
    "\n",
    "# List of metadata columns (everything except date, UUID, and prices)\n",
    "metadata_cols = [col for col in ndf.columns if col not in [\"date\", \"station_uuid\"] + price_cols]\n",
    "\n",
    "# For each station: resample time series and attach metadata\n",
    "for station_id, group in ndf.groupby(\"station_uuid\"):\n",
    "    group = group.set_index(\"date\").sort_index()\n",
    "\n",
    "    # Resample prices (forward fill) for each fuel type\n",
    "    reindexed_prices = {\n",
    "        col: group[col].reindex(quarter_hours, method=\"ffill\")\n",
    "        for col in price_cols\n",
    "    }\n",
    "\n",
    "    # DataFrame with timestamps and UUID\n",
    "    result = pd.DataFrame({\n",
    "        \"date\": quarter_hours,\n",
    "        \"station_uuid\": station_id,\n",
    "        **{col: reindexed_prices[col].values for col in price_cols}\n",
    "    })\n",
    "\n",
    "    # Metadata from the first row (if available)\n",
    "    meta = group[metadata_cols].iloc[0] if not group.empty else pd.Series(index=metadata_cols)\n",
    "\n",
    "    # Attach metadata to each row\n",
    "    for col in metadata_cols:\n",
    "        result[col] = meta[col]\n",
    "\n",
    "    resampled_list.append(result)\n",
    "\n",
    "# Merge all resampled data\n",
    "resampled_df = pd.concat(resampled_list, ignore_index=True)\n",
    "\n",
    "# Restrict to April\n",
    "resampled_df = resampled_df[resampled_df[\"date\"] >= pd.Timestamp(\"2025-04-01 00:00:00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9edd1fe4-81f0-49a2-a1b6-c7344d87fa70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing metadata\n",
    "df_clean = resampled_df[resampled_df[[\"latitude\", \"longitude\"]].notna().all(axis=1)].copy()\n",
    "\n",
    "# Remove rows with all prices missing\n",
    "df_clean = df_clean[df_clean[[\"diesel\", \"e5\", \"e10\"]].notna().any(axis=1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs after filter: 2963412\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"date\"] = pd.to_datetime(df_clean[\"date\"])\n",
    "\n",
    "# 1) Last observation date per station\n",
    "last_seen = (\n",
    "    df_clean.groupby(\"station_uuid\", as_index=False)[\"date\"]\n",
    "    .max()\n",
    "    .rename(columns={\"date\": \"last_seen\"})\n",
    ")\n",
    "\n",
    "# 2) Create a new DataFrame\n",
    "df_filtered = df_clean.merge(last_seen, on=\"station_uuid\", how=\"left\")\n",
    "df_filtered[\"last_seen\"] = pd.to_datetime(df_filtered[\"last_seen\"])\n",
    "\n",
    "# 3) Moving activity and price filter\n",
    "cutoff = pd.Timedelta(days=15)\n",
    "mask_active = (df_filtered[\"date\"] - df_filtered[\"last_seen\"]) <= cutoff\n",
    "mask_price  = df_filtered[\"diesel\"] > 0\n",
    "\n",
    "df_filtered = df_filtered[mask_active & mask_price].copy()\n",
    "\n",
    "print(\"Obs after filter:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5359d050-b812-4ec5-a774-dd4fe950a659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace 0 with NaN for all price columns\n",
    "price_columns = [\"diesel\", \"e5\", \"e10\"]\n",
    "\n",
    "# Replace 0 and negative values with NA in price columns (if they exist)\n",
    "for col in price_columns:\n",
    "    if col in df_filtered.columns:\n",
    "        df_filtered[col] = df_filtered[col].apply(lambda x: pd.NA if x <= 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "201f19e1-fb55-45fd-96d9-01fba67783af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handling opening times\n",
    "def is_open(opening_json, timestamp):\n",
    "    try:\n",
    "        if opening_json.strip() == \"{}\":\n",
    "            return True  # always open according to MTS-K\n",
    "\n",
    "        data = json.loads(opening_json)\n",
    "        weekday = timestamp.weekday()  # 0 = Monday, ..., 6 = Sunday\n",
    "        time_str = timestamp.strftime(\"%H:%M\")\n",
    "\n",
    "        for block in data.get(\"openingTimes\", []):\n",
    "            if block.get(\"applicable_days\", 0) & (1 << weekday):\n",
    "                for period in block.get(\"periods\", []):\n",
    "                    if period[\"startp\"] <= time_str < period[\"endp\"]:\n",
    "                        return True\n",
    "        return False  # none of the time blocks apply\n",
    "    except:\n",
    "        return True  # if uncertain: treat as open\n",
    "\n",
    "# Create a copy\n",
    "df_clean_opening = df_filtered.copy()\n",
    "\n",
    "# Price columns to set to NaN if station is closed\n",
    "price_columns = [col for col in [\"diesel\", \"e5\", \"e10\"] if col in df_clean_opening.columns]\n",
    "\n",
    "# Calculate opening status for each row\n",
    "df_clean_opening[\"is_open\"] = df_clean_opening.apply(\n",
    "    lambda row: is_open(row[\"openingtimes_json\"], row[\"date\"]), axis=1\n",
    ")\n",
    "\n",
    "# Set all prices to NaN if closed\n",
    "for col in price_columns:\n",
    "    df_clean_opening.loc[~df_clean_opening[\"is_open\"], col] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected highway stations: 452\n",
      "Rows before exclusion: 2963412\n",
      "Rows after exclusion: 2868636\n"
     ]
    }
   ],
   "source": [
    "# Verification and exclusion of highway stations using the stations dataset\n",
    "autobahn_pattern = (\n",
    "    r\"\\b(?:\"\n",
    "    r\"autobahn(?:\\s*tankstelle)?\"\n",
    "    r\"|autohof\"\n",
    "    r\"|rasthof\"\n",
    "    r\"|rast[-\\s]?stätte\"\n",
    "    r\"|raststaette\"\n",
    "    r\"|tank\\s*(?:und|&)\\s*rast\"\n",
    "    r\"|service\\s*area\"\n",
    "    r\"|an\\s+der\\s+a[0-9]{1,2}\"\n",
    "    r\"|a\\s*[0-9]{1,2}\"\n",
    "    r\"|ausfahrt\"\n",
    "    r\"|abfahrt\"\n",
    "    r\"|bundesautobahn\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "# 2. Identify highway stations by name or street\n",
    "is_highway = (\n",
    "    stations_latest[\"name\"].str.contains(autobahn_pattern, case=False, na=False) |\n",
    "    stations_latest[\"street\"].str.contains(autobahn_pattern, case=False, na=False)\n",
    ")\n",
    "\n",
    "# 3. Get UUIDs of identified highway stations\n",
    "highway_ids = stations_latest[is_highway][\"uuid\"].unique()\n",
    "\n",
    "# 4. Exclude these stations from df_clean_opening\n",
    "df_clean_opening_excl_highway = df_clean_opening[\n",
    "    ~df_clean_opening[\"station_uuid\"].isin(highway_ids)\n",
    "].copy()\n",
    "\n",
    "# 5. Output\n",
    "print(\"Number of detected highway stations:\", len(highway_ids))\n",
    "print(\"Rows before exclusion:\", len(df_clean_opening))\n",
    "print(\"Rows after exclusion:\", len(df_clean_opening_excl_highway))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date  weekday  holiday  time_sin  time_cos\n",
      "0 2025-04-01 00:00:00        1        0  0.000000  1.000000\n",
      "1 2025-04-01 00:15:00        1        0  0.065403  0.997859\n",
      "2 2025-04-01 00:30:00        1        0  0.130526  0.991445\n",
      "3 2025-04-01 00:45:00        1        0  0.195090  0.980785\n",
      "4 2025-04-01 01:00:00        1        0  0.258819  0.965926\n"
     ]
    }
   ],
   "source": [
    "# Time/Day Features\n",
    "df = df_clean_opening_excl_highway.copy()\n",
    "\n",
    "# Assuming your DataFrame is named df and the timestamp column is 'date'\n",
    "s = df['date'].astype(str) \\\n",
    "              .str.replace('\\u00a0', ' ', regex=False) \\\n",
    "              .str.strip() \\\n",
    "              .str.replace(r'\\s+', ' ', regex=True)\n",
    "parsed_full  = pd.to_datetime(s, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "parsed_date  = pd.to_datetime(s, format='%Y-%m-%d', errors='coerce')\n",
    "df['date']   = parsed_full.fillna(parsed_date)\n",
    "\n",
    "# 1) Weekday (Monday–Sunday)\n",
    "df[\"weekday\"] = df[\"date\"].dt.weekday.astype(int)\n",
    "\n",
    "# 2) Holiday dummy (1 if date is 2025-04-18 or 2025-04-21, otherwise 0)\n",
    "holidays = {pd.Timestamp('2025-04-18'), pd.Timestamp('2025-04-21')}\n",
    "# .normalize() resets the time to midnight so you compare only dates\n",
    "df['holiday'] = df['date'].dt.normalize().isin(holidays).astype(int)\n",
    "\n",
    "# 3) Time formatted as \"hh:mm\"\n",
    "minutes = df[\"date\"].dt.hour * 60 + df[\"date\"].dt.minute\n",
    "df[\"time_sin\"] = np.sin(2 * np.pi * minutes / 1440)\n",
    "df[\"time_cos\"] = np.cos(2 * np.pi * minutes / 1440)\n",
    "\n",
    "\n",
    "# Example output\n",
    "print(df[['date', 'weekday', 'holiday', 'time_sin', 'time_cos']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_brand(df, brand_col=\"brand\", new_col=\"brand_cat\"):\n",
    "    # create new variable\n",
    "    df[new_col] = df[brand_col].copy()\n",
    "\n",
    "    # 1) Direct renaming\n",
    "    df.loc[df[new_col] == \"AGIP ENI\", new_col] = \"Agip\"\n",
    "    df.loc[df[new_col] == \"AVIA XPress\", new_col] = \"AVIA\"\n",
    "    df.loc[df[new_col] == \"ORLEN Express\", new_col] = \"OLEN\"\n",
    "    df.loc[df[new_col] == \"SB-Markttankstelle\", new_col] = \"SB\"\n",
    "    df.loc[df[new_col] == \"TotalEnergies Truckstop\", new_col] = \"TotalEnergies\"\n",
    "\n",
    "    # 2) Starts-with rules (case-insensitive where useful)\n",
    "    mask_bft = df[new_col].str.lower().str.startswith(\"bft\", na=False)\n",
    "    df.loc[mask_bft, new_col] = \"bft\"\n",
    "\n",
    "    mask_hmh = df[new_col].str.startswith(\"HMH MineralÃ¶l Service\", na=False)\n",
    "    df.loc[mask_hmh, new_col] = \"HMH\"\n",
    "\n",
    "    mask_raiff = df[new_col].str.startswith(\"Raiffeisen\", na=False)\n",
    "    df.loc[mask_raiff, new_col] = \"Raiffeisen\"\n",
    "\n",
    "    mask_frei = df[new_col].str.lower().str.startswith(\"frei\", na=False)\n",
    "    df.loc[mask_frei, new_col] = np.nan\n",
    "\n",
    "    # 3) Remove all \"supermarket\" cases\n",
    "    mask_supermarkt_tank = df[new_col].str.contains(\"Supermarkt-Tankstelle\", na=False)\n",
    "    mask_supermarkt = df[new_col].str.contains(\"Supermarkt\", na=False)\n",
    "\n",
    "    df.loc[mask_supermarkt_tank | mask_supermarkt, new_col] = np.nan\n",
    "\n",
    "    # 4) Remove all brands that appear only once\n",
    "    counts = df[new_col].value_counts(dropna=True)\n",
    "    singletons = counts[counts == 1].index\n",
    "    df.loc[df[new_col].isin(singletons), new_col] = np.nan\n",
    "    \n",
    "    df[new_col] = df[new_col].astype(\"category\").cat.codes.astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = clean_brand(df, brand_col=\"brand\", new_col=\"brand_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw oil prices via API\n",
    "# Because of a missing network connection here, this step was run on a different server\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Retrieve hourly Brent data from April 1 to April 30, 2025\n",
    "ticker = yf.Ticker(\"BZ=F\")\n",
    "df = ticker.history(start=\"2025-04-01\", end=\"2025-05-01\", interval=\"60m\")\n",
    "df = df.reset_index()\n",
    "df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
    "\n",
    "# 2. Generate a complete 15-minute timestamp grid for April 2025\n",
    "all_times = pd.date_range(\n",
    "    start=\"2025-04-01 00:00:00\",\n",
    "    end=\"2025-04-30 23:45:00\",\n",
    "    freq=\"15min\"\n",
    ")\n",
    "full_df = pd.DataFrame({'Datetime': all_times})\n",
    "\n",
    "# 3. Select only the 'Open' price and rename it to 'Brent_Price'\n",
    "df_short = df[['Datetime', 'Open']].rename(columns={'Open': 'Brent_Price'})\n",
    "\n",
    "# 4. Merge the full 15-minute grid with the hourly prices,\n",
    "#    filling each 15-min slot with the most recent (backward fill) hourly price\n",
    "merged = pd.merge_asof(\n",
    "    full_df.sort_values('Datetime'),\n",
    "    df_short.sort_values('Datetime'),\n",
    "    on='Datetime',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# 5. Save the result to a CSV file\n",
    "merged.to_csv(\"brent_april_2025_15min_filled.csv\", index=False)\n",
    "\n",
    "# Optional: display the first 20 rows and total count (should be 2880 timestamps)\n",
    "print(merged.head(20))\n",
    "print(\"Total number of timestamps:\", len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (restore from saved file)\n",
    "brent = pd.read_csv(\"/gpfs/scratch/qusta100/STGNN/Data/Temp/brent_april_2025_15min_filled.csv\")\n",
    "brent = brent.rename(columns={\"Datetime\": \"date\"})\n",
    "brent['date'] = pd.to_datetime(brent['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_uuid</th>\n",
       "      <th>diesel</th>\n",
       "      <th>e5</th>\n",
       "      <th>e10</th>\n",
       "      <th>uuid</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>street</th>\n",
       "      <th>house_number</th>\n",
       "      <th>...</th>\n",
       "      <th>in_thuringia</th>\n",
       "      <th>in_region</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>is_open</th>\n",
       "      <th>weekday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>time_sin</th>\n",
       "      <th>time_cos</th>\n",
       "      <th>brand_cat</th>\n",
       "      <th>Brent_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-01 00:15:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>0.997859</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-01 00:30:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.130526</td>\n",
       "      <td>0.991445</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-01 00:45:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195090</td>\n",
       "      <td>0.980785</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.649</td>\n",
       "      <td>00060065-7890-4444-8888-acdc00000004</td>\n",
       "      <td>Georg Ultsch GmbH</td>\n",
       "      <td>Tankstelle Lichtenfels</td>\n",
       "      <td>Robert-Koch-Str.</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 21:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.959999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                          station_uuid  diesel     e5  \\\n",
       "0 2025-04-01 00:00:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "1 2025-04-01 00:15:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "2 2025-04-01 00:30:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "3 2025-04-01 00:45:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "4 2025-04-01 01:00:00  00060065-7890-4444-8888-acdc00000004   1.559  1.709   \n",
       "\n",
       "     e10                                  uuid               name  \\\n",
       "0  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "1  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "2  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "3  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "4  1.649  00060065-7890-4444-8888-acdc00000004  Georg Ultsch GmbH   \n",
       "\n",
       "                    brand            street house_number  ... in_thuringia  \\\n",
       "0  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...        False   \n",
       "1  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...        False   \n",
       "2  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...        False   \n",
       "3  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...        False   \n",
       "4  Tankstelle Lichtenfels  Robert-Koch-Str.           18  ...        False   \n",
       "\n",
       "  in_region           last_seen  is_open weekday holiday  time_sin  time_cos  \\\n",
       "0         1 2025-04-30 21:45:00     True       1       0  0.000000  1.000000   \n",
       "1         1 2025-04-30 21:45:00     True       1       0  0.065403  0.997859   \n",
       "2         1 2025-04-30 21:45:00     True       1       0  0.130526  0.991445   \n",
       "3         1 2025-04-30 21:45:00     True       1       0  0.195090  0.980785   \n",
       "4         1 2025-04-30 21:45:00     True       1       0  0.258819  0.965926   \n",
       "\n",
       "  brand_cat  Brent_Price  \n",
       "0      74.0    74.959999  \n",
       "1      74.0    74.959999  \n",
       "2      74.0    74.959999  \n",
       "3      74.0    74.959999  \n",
       "4      74.0    74.959999  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with main data set (df_main)\n",
    "final_df = pd.merge(df, brent, on='date', how='left')\n",
    "\n",
    "# Show data set\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataframe to CSV\n",
    "final_df.to_csv(\"/gpfs/scratch/qusta100/STGNN/Data/Temp/final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
